{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\new folder\\anaconda\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in d:\\new folder\\anaconda\\lib\\site-packages (from nltk) (1.12.0)\n",
      "[\"hello my self akash sharma and i am from phagwara and i am pursunig my b tech degree fom gna university in third year but stil i don't know ho to apply bubble sort in any language in any language even in python\"]\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "#nltk.download()\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import  sent_tokenize, word_tokenize\n",
    "a=\"hello my self akash sharma and i am from phagwara and i am pursunig my b tech degree fom gna university in third year but stil i don't know ho to apply bubble sort in any language in any language even in python\"\n",
    "print(sent_tokenize(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans\n",
      "Requirement already satisfied: requests in d:\\new folder\\anaconda\\lib\\site-packages (from googletrans) (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\new folder\\anaconda\\lib\\site-packages (from requests->googletrans) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\new folder\\anaconda\\lib\\site-packages (from requests->googletrans) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\new folder\\anaconda\\lib\\site-packages (from requests->googletrans) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\new folder\\anaconda\\lib\\site-packages (from requests->googletrans) (1.24.2)\n",
      "Installing collected packages: googletrans\n",
      "Successfully installed googletrans-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated(src=hi, dest=en, text=He's always happy, pronunciation=None, extra_data=\"{'translat...\")\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator \n",
    "translator=Translator()\n",
    "result=translator.translate(\"Akashay hmesha khush rehta hai\")\n",
    "print(result)\n",
    "#result=translator.translate(\"Ram khana khane wala hai\")\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m\n",
      "y\n",
      " \n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      " \n",
      "a\n",
      "k\n",
      "a\n",
      "s\n",
      "h\n",
      " \n",
      "s\n",
      "h\n",
      "a\n",
      "r\n",
      "m\n",
      "a\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "i\n",
      " \n",
      "a\n",
      "m\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "p\n",
      "h\n",
      "a\n",
      "g\n",
      "w\n",
      "a\n",
      "r\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "a=\"my self akash sharma and and i am from phagwara\"\n",
    "b=a.split(\".\")\n",
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in d:\\anaconda\\lib\\site-packages (2020.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tiger'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "a=\"Tiger is the national anmimal of india\"\n",
    "b=\"Tiger\"\n",
    "re.match(b,a).group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tiger'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=\"Tiger is the national anmimal of india\"\n",
    "b=\"Tiger\"\n",
    "re.search(b,a).group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['national', 'national']\n"
     ]
    }
   ],
   "source": [
    "a=\"national animal is tiger and national bird is pecock\"\n",
    "b=\"national\"\n",
    "c=re.findall(b,a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "a=\"national animal is tiger and national bird is pecock\"\n",
    "b=\"national\"\n",
    "c=re.finditer(b,a)\n",
    "for x in c:\n",
    "    print(x.start())\n",
    "#here it is returing the index number of n of national word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"national animal is tiger and national bird is pecock\"\n",
    "b=\"national\"\n",
    "re.findall(b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12-05-1999', '11-11-2000']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=\"abac 65433564,12-05-1999,11-11-2000\"\n",
    "b=r'\\d{2}-\\d{2}-\\d{4}'\n",
    "re.findall(b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is', 'a', 'sample', 'text', 'string']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=\"this is;a,sample,text,string\"\n",
    "b=r'[;,\\a]'\n",
    "re.split(b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hockey is the national sport of India'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string=\"Cricket is the national sport of India\"\n",
    "replacer=\"Cricket\"\n",
    "replacement=\"Hockey\"\n",
    "re.sub(replacer,replacement,string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hockey is the national sport of India'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string=\"Cricket is the national sport of India\"\n",
    "replacer=\"Cricket\"\n",
    "re.sub(replacer,\"Hockey\",string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'self',\n",
       " 'Akash',\n",
       " 'Sharma',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'from',\n",
       " 'Phagwara',\n",
       " '.',\n",
       " 'I',\n",
       " 'study',\n",
       " 'at',\n",
       " 'GNA',\n",
       " 'University']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization,word tokenization\n",
    "a=\"My self Akash Sharma. I am from Phagwara. I study at GNA University\"\n",
    "sent_tokenize(a)\n",
    "word_tokenize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer= PorterStemmer()\n",
    "print(stemmer.stem(\"running\"))\n",
    "print(stemmer.stem(\"runs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "#stemming with loop\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer= PorterStemmer()\n",
    "a=[\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "for w in a:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increasing\n",
      "increase\n"
     ]
    }
   ],
   "source": [
    "#lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "print(lemm.lemmatize(\"increasing\", pos=\"n\"))\n",
    "#pos stands for part of speech and we have passed the noun participle\n",
    "print(lemm.lemmatize(\"increasing\", pos=\"v\"))\n",
    "# we have passed the verb participle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My', 'PRP$'),\n",
       " ('self', 'NN'),\n",
       " ('Akash', 'NNP'),\n",
       " ('Sharma', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('from', 'IN'),\n",
       " ('Phagwara', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('study', 'VBP'),\n",
       " ('at', 'IN'),\n",
       " ('GNA', 'NNP'),\n",
       " ('University', 'NNP')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pos tags\n",
    "from nltk import pos_tag\n",
    "a=\"My self Akash Sharma. I am from Phagwara. I study at GNA University\"\n",
    "token=word_tokenize(a)\n",
    "#here we firstly did the word tokenize and stored in the token refrence\n",
    "pos_tag(token)\n",
    "#here we did the part of speech yags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('love.n.01'),\n",
       " Synset('love.n.02'),\n",
       " Synset('beloved.n.01'),\n",
       " Synset('love.n.04'),\n",
       " Synset('love.n.05'),\n",
       " Synset('sexual_love.n.02'),\n",
       " Synset('love.v.01'),\n",
       " Synset('love.v.02'),\n",
       " Synset('love.v.03'),\n",
       " Synset('sleep_together.v.01')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#synoniams of a word\n",
    "from nltk.corpus import wordnet\n",
    "wordnet.synsets(\"love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Akash', 'Sharma', 'from')\n",
      "('Sharma', 'from', 'Phagwara')\n"
     ]
    }
   ],
   "source": [
    "#n grams\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "a=\"Akash Sharma from Phagwara\"\n",
    "n=3\n",
    "#number of n grams\n",
    "for gram in ngrams(word_tokenize(a),n):\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'both', 'her', \"it's\", 'will', 'couldn', 'am', 'she', 'over', 'from', 'd', 'his', 'having', \"couldn't\", \"hasn't\", 'nor', 'down', 'before', 'ours', 'with', 'so', \"hadn't\", 'do', 'why', 'other', 'how', 'but', 'while', 'any', 'to', 'most', 'i', 'myself', 'should', 'on', \"isn't\", 'you', \"you've\", 's', \"wouldn't\", \"mustn't\", 'our', 'yourself', 'the', 'it', 'been', 'have', 'if', \"she's\", 'after', 'had', 'don', 'me', 'is', 'by', 'some', 'for', 'through', \"wasn't\", 'y', 'too', 'doesn', \"you'd\", 'being', 'all', 'was', \"haven't\", 'below', 'or', \"you'll\", 'a', 'above', 'then', 'once', 'until', 'o', 'that', 'did', 'doing', 'my', 'll', \"you're\", 'he', 'few', 'can', 'weren', 're', 'theirs', 'won', \"shouldn't\", \"aren't\", 'of', 'hasn', 'only', 'which', 'hers', 'their', 'who', 'own', \"don't\", \"won't\", 'm', 'into', 'now', 'there', 'your', \"should've\", 'here', 'ma', \"weren't\", 'again', 'very', 'herself', 'just', 'were', 'out', 'further', 'this', 'itself', 've', 'be', 'those', 'and', 'same', 'where', 'wouldn', 'mustn', 'such', \"mightn't\", 'off', 't', 'than', 'an', 'himself', 'against', 'between', 'up', 'about', 'as', 'haven', 'these', 'at', 'hadn', 'does', 'has', 'them', 'its', 'more', 'when', 'aren', 'yours', 'under', 'him', 'needn', 'yourselves', 'we', 'because', 'didn', 'wasn', \"shan't\", \"that'll\", 'no', 'shan', 'whom', \"doesn't\", 'ourselves', 'in', 'ain', 'themselves', 'each', 'isn', \"didn't\", 'not', 'what', 'mightn', 'during', \"needn't\", 'are', 'shouldn', 'they'}\n"
     ]
    }
   ],
   "source": [
    "#stop words\n",
    "from nltk.corpus import stopwords\n",
    "b=set(stopwords.words(\"english\"))\n",
    "#declaring a set  of english stopwords\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'self', 'akash', 'sharma', 'phagwara', 'study', 'GNA', 'University', 'computer', 'science', 'degree', 'computer', 'science']\n"
     ]
    }
   ],
   "source": [
    "#removing stop words from the sentences\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "b=set(stopwords.words(\"english\"))\n",
    "a=\"Hello my self akash sharma and i am from phagwara i study at GNA University with a computer science degree in computer science\"\n",
    "x=word_tokenize(a)\n",
    "#print(x)\n",
    "filtered_sentences = []\n",
    "for m in x:\n",
    "    if m not in b:\n",
    "        filtered_sentences.append(m)\n",
    "print(filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'self', 'akash', 'sharma', 'phagwara', 'study', 'GNA', 'University', 'computer', 'science', 'degree', 'computer', 'science']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "b=set(stopwords.words(\"english\"))\n",
    "a=\"Hello my self akash sharma and i am from phagwara i study at GNA University with a computer science degree in computer science\"\n",
    "x=word_tokenize(a)\n",
    "filtered_sentences = [m for m in x  if not m in b]\n",
    "#one liner\n",
    "print(filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
